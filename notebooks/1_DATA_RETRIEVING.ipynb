{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b5ec37e-3ea2-4410-b49b-d89f82627005",
   "metadata": {},
   "source": [
    "# NMVCCS Actuarial Field Extraction\n",
    "\n",
    "## Description\n",
    "This notebook extracts relevant data for actuarial analysis from NMVCCS (National Motor Vehicle Crash Causation Survey) XML and HTML files.\n",
    "\n",
    "## Features\n",
    "- **XML Parsing**: Structured extraction from NMVCCS XML files\n",
    "- **HTML Parsing**: Web scraping from 508-compliant HTML files\n",
    "- **Bulk Processing**: Processing directories containing thousands of files\n",
    "- **Structured Output**: CSV with 50+ actuarial variables per case\n",
    "\n",
    "## Input\n",
    "- Single XML files or directories\n",
    "- Single HTML files or directories  \n",
    "- Mixed directories (XML + HTML)\n",
    "\n",
    "## Output\n",
    "- `output.csv`: Raw extracted data\n",
    "- `DB.csv`: Final database with `;` separator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264c49b9-e406-4bd2-acb3-a03b475adc37",
   "metadata": {},
   "source": [
    "## Package Installation\n",
    "\n",
    "Installing necessary packages for data extraction:\n",
    "- **BeautifulSoup4**: HTML parsing\n",
    "- **html2text**: HTML to text conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9467cbb4-4721-4037-b1b2-f391d9b26b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /venv/main/lib/python3.12/site-packages (0.0.2)\n",
      "Collecting html2text\n",
      "  Downloading html2text-2025.4.15-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /venv/main/lib/python3.12/site-packages (from bs4) (4.13.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /venv/main/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /venv/main/lib/python3.12/site-packages (from beautifulsoup4->bs4) (4.13.2)\n",
      "Downloading html2text-2025.4.15-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: html2text\n",
      "Successfully installed html2text-2025.4.15\n"
     ]
    }
   ],
   "source": [
    "# !pip install bs4 html2text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d39383-a6e0-4e04-83df-c5a601c59e8b",
   "metadata": {},
   "source": [
    "## Library Imports\n",
    "\n",
    "Importing all necessary libraries for processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5844c6-4494-4a02-b548-fc438fc8dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7638daab-6c69-4c9c-8df5-73e38791732a",
   "metadata": {},
   "source": [
    "## XML Data Extraction\n",
    "\n",
    "### Variables Extracted from XML\n",
    "The function extracts the following data categories:\n",
    "\n",
    "| Category | Variables | Description |\n",
    "|----------|-----------|-------------|\n",
    "| **Case Info** | CaseID, CaseStr, NumOfVehicle | Case identifiers |\n",
    "| **Summary** | CaseSummary | Textual case description |\n",
    "| **Crash** | CrashTime, CrashSeverity | Temporal details and severity |\n",
    "| **Vehicles** | Make, Model, Year, Odometer | Vehicle characteristics |\n",
    "| **Tires** | TireDepth, TirePressure | Tire conditions |\n",
    "| **Damage** | DamageExtent | Damage extent (CDC) |\n",
    "| **Drivers** | Age, Sex, InjurySeverity | Demographics and injuries |\n",
    "| **Pre-crash** | CriticalEvent, CriticalReason | Critical events |\n",
    "| **Environment** | RoadSurface, Atmospheric | Environmental conditions |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "676e74ec-c721-4833-be44-17be7098dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_actuarial_data_from_xml(xml_file_or_directory, output_csv=None):\n",
    "    \"\"\"\n",
    "    Estrae dati rilevanti per analisi attuariale dai file XML NMVCCS.\n",
    "    \n",
    "    Args:\n",
    "        xml_file_or_directory: Un file XML singolo o una directory contenente file XML\n",
    "        output_csv: Opzionale, percorso dove salvare il CSV risultante\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame pandas con i dati estratti\n",
    "    \"\"\"\n",
    "    # Determina se l'input è un file o una directory\n",
    "    if os.path.isfile(xml_file_or_directory):\n",
    "        xml_files = [xml_file_or_directory]\n",
    "    elif os.path.isdir(xml_file_or_directory):\n",
    "        xml_files = glob.glob(os.path.join(xml_file_or_directory, \"*.xml\"))\n",
    "    else:\n",
    "        raise ValueError(\"Il percorso fornito non è né un file né una directory valida\")\n",
    "    \n",
    "    # Lista per memorizzare i dati estratti\n",
    "    data_list = []\n",
    "    \n",
    "    # Processa ogni file XML\n",
    "    for xml_file in tqdm(xml_files, desc=\"Elaborazione file XML\"):\n",
    "        try:\n",
    "            # Parsing del file XML\n",
    "            tree = ET.parse(xml_file)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            # Dizionario per memorizzare i dati di questo caso\n",
    "            case_data = {}\n",
    "            \n",
    "            # Estrai informazioni di base del caso\n",
    "            case_data['CaseID'] = root.get('CaseID')\n",
    "            case_data['CaseStr'] = root.get('CaseStr')\n",
    "            case_data['NumOfVehicle'] = root.get('NumOfVehicle')\n",
    "            \n",
    "            # Estrai il riassunto del caso (summary)\n",
    "            summary_elem = root.find(\".//XML_CASESUMMARY/SUMMARY\")\n",
    "            if summary_elem is not None and summary_elem.text:\n",
    "                # Pulisci il testo rimuovendo spazi extra e andando a capo\n",
    "                summary_text = re.sub(r'\\s+', ' ', summary_elem.text).strip()\n",
    "                case_data['CaseSummary'] = summary_text\n",
    "            \n",
    "            # Estrai informazioni sul tempo e data dell'incidente\n",
    "            crash_derived = root.find(\".//XML_CRASHDERIVED\")\n",
    "            if crash_derived is not None:\n",
    "                for elem in crash_derived:\n",
    "                    case_data[elem.tag] = elem.text\n",
    "            \n",
    "            # Estrai informazioni sull'evento di crash\n",
    "            crash_elem = root.find(\".//CRASH\")\n",
    "            if crash_elem is not None:\n",
    "                case_data['CrashTime'] = crash_elem.find(\"TIME\").text if crash_elem.find(\"TIME\") is not None else None\n",
    "                kabcou_elem = crash_elem.find(\"KABCOU\")\n",
    "                if kabcou_elem is not None:\n",
    "                    case_data['CrashSeverity'] = kabcou_elem.get('attrCatStr')\n",
    "                    case_data['CrashSeverityCode'] = kabcou_elem.get('value')\n",
    "            \n",
    "            # Estrai informazioni per ogni veicolo coinvolto\n",
    "            for i, vehicle_elem in enumerate(root.findall(\".//GeneralVehicle\"), 1):\n",
    "                vehicle_id = vehicle_elem.get('VEHICLEID')\n",
    "                \n",
    "                # Informazioni base del veicolo\n",
    "                vehicle = vehicle_elem.find(\".//VEHICLE\")\n",
    "                if vehicle is not None:\n",
    "                    case_data[f'Vehicle{i}_Year'] = vehicle.find(\"MODELYEAR\").text if vehicle.find(\"MODELYEAR\") is not None else None\n",
    "                    \n",
    "                    make_elem = vehicle.find(\"MAKE\")\n",
    "                    if make_elem is not None:\n",
    "                        case_data[f'Vehicle{i}_Make'] = make_elem.get('attrCatStr')\n",
    "                    \n",
    "                    model_elem = vehicle.find(\"MODEL\")\n",
    "                    if model_elem is not None:\n",
    "                        case_data[f'Vehicle{i}_Model'] = model_elem.get('attrCatStr')\n",
    "                    \n",
    "                    body_type_elem = vehicle.find(\"BODY_TYPE\")\n",
    "                    if body_type_elem is not None:\n",
    "                        case_data[f'Vehicle{i}_BodyType'] = body_type_elem.get('attrCatStr')\n",
    "                    \n",
    "                    case_data[f'Vehicle{i}_Odometer'] = vehicle.find(\"ODOMETER\").text if vehicle.find(\"ODOMETER\") is not None else None\n",
    "                \n",
    "                # Informazioni sui pneumatici\n",
    "                tires = vehicle_elem.findall(\".//TIRE\")\n",
    "                for tire in tires:\n",
    "                    tire_loc = tire.get('TIRE_LOCATION')\n",
    "                    tire_depth = tire.find(\"TIRE_TREAD_DEPTH\")\n",
    "                    if tire_depth is not None:\n",
    "                        case_data[f'Vehicle{i}_Tire{tire_loc}_Depth'] = tire_depth.text\n",
    "                    \n",
    "                    tire_pressure = tire.find(\"TIRE_PRESSURE\")\n",
    "                    if tire_pressure is not None and tire_pressure.text and tire_pressure.text.isdigit():\n",
    "                        case_data[f'Vehicle{i}_Tire{tire_loc}_Pressure'] = tire_pressure.text\n",
    "                \n",
    "                # CDC (Collision Deformation Classification)\n",
    "                cdc = vehicle_elem.find(\".//CDC\")\n",
    "                if cdc is not None:\n",
    "                    damage_extent = cdc.find(\"DAMAGEEXTENT\")\n",
    "                    if damage_extent is not None:\n",
    "                        case_data[f'Vehicle{i}_DamageExtent'] = damage_extent.get('attrCatStr')\n",
    "                \n",
    "                # Informazioni sull'autista\n",
    "                driver = root.find(f\".//Occupant[@VEHICLEID='{vehicle_id}']/OccupantByVehicle/OCCUPANT[ROLE='1']\")\n",
    "                if driver is not None:\n",
    "                    case_data[f'Vehicle{i}_DriverAge'] = driver.find(\"AGE\").text if driver.find(\"AGE\") is not None else None\n",
    "                    \n",
    "                    sex_elem = driver.find(\"SEX_PREGNANCY\")\n",
    "                    if sex_elem is not None:\n",
    "                        case_data[f'Vehicle{i}_DriverSex'] = sex_elem.get('attrCatStr')\n",
    "                    \n",
    "                    kabcou_elem = driver.find(\"KABCOU\")\n",
    "                    if kabcou_elem is not None:\n",
    "                        case_data[f'Vehicle{i}_DriverInjurySeverity'] = kabcou_elem.get('attrCatStr')\n",
    "                \n",
    "                # Informazioni precrash per il veicolo\n",
    "                precrash = root.find(f\".//PrecrashAssessmentForm[@VEHICLEID='{vehicle_id}']\")\n",
    "                if precrash is not None:\n",
    "                    # Critical Event\n",
    "                    critical_event = precrash.find(\".//PRECRASH/CRITICAL_EVENT\")\n",
    "                    if critical_event is not None:\n",
    "                        case_data[f'Vehicle{i}_CriticalEvent'] = critical_event.get('attrCatStr')\n",
    "                    \n",
    "                    # Critical Reason\n",
    "                    critical_reason = precrash.find(\".//PRECRASH/CRITICAL_REASON\")\n",
    "                    if critical_reason is not None:\n",
    "                        case_data[f'Vehicle{i}_CriticalReason'] = critical_reason.get('attrCatStr')\n",
    "                    \n",
    "                    # Driver Experience\n",
    "                    route_freq = precrash.find(\".//DRIVER_BEHAVIOR/THIS_ROUTE_FREQUENCY\")\n",
    "                    if route_freq is not None:\n",
    "                        case_data[f'Vehicle{i}_RouteFrequency'] = route_freq.get('attrCatStr')\n",
    "                    \n",
    "                    # Driver Fatigue\n",
    "                    fatigue = precrash.find(\".//FATIGUE/DRIVER_FATIGUE\")\n",
    "                    if fatigue is not None:\n",
    "                        case_data[f'Vehicle{i}_DriverFatigue'] = fatigue.get('attrCatStr')\n",
    "                    \n",
    "                    # Alcohol/Drug\n",
    "                    alcohol_test = precrash.find(\".//DRIVER_HEALTH/ALCOHOL_TEST_RESULT\")\n",
    "                    if alcohol_test is not None:\n",
    "                        case_data[f'Vehicle{i}_AlcoholTest'] = alcohol_test.get('attrCatStr')\n",
    "                    \n",
    "                    # Distraction/Inattention\n",
    "                    surveillance = precrash.find(\".//DRIVER_BEHAVIOR/SURVEILLANCE\")\n",
    "                    if surveillance is not None:\n",
    "                        case_data[f'Vehicle{i}_Surveillance'] = surveillance.get('attrCatStr')\n",
    "            \n",
    "            # Estrai informazioni sull'ambiente/strada\n",
    "            roadway = root.find(\".//ROADWAY\")\n",
    "            if roadway is not None:\n",
    "                surface_type = roadway.find(\"SURFACE_TYPE\")\n",
    "                if surface_type is not None:\n",
    "                    case_data['RoadSurfaceType'] = surface_type.get('attrCatStr')\n",
    "                \n",
    "                surface_cond = roadway.find(\"SURFACE_CONDITION\")\n",
    "                if surface_cond is not None:\n",
    "                    case_data['RoadSurfaceCondition'] = surface_cond.get('attrCatStr')\n",
    "                \n",
    "                roadway_align = roadway.find(\"ROADWAY_ALIGN\")\n",
    "                if roadway_align is not None:\n",
    "                    case_data['RoadwayAlignment'] = roadway_align.get('attrCatStr')\n",
    "                \n",
    "                roadway_profile = roadway.find(\"ROADWAY_VERT_PROFILE\")\n",
    "                if roadway_profile is not None:\n",
    "                    case_data['RoadwayVerticalProfile'] = roadway_profile.get('attrCatStr')\n",
    "            \n",
    "            # Estrai informazioni sulle condizioni atmosferiche\n",
    "            atmospheric = root.find(\".//ATMOSPHERIC_CONDITION/ATMOSPHERICCONDITION\")\n",
    "            if atmospheric is not None:\n",
    "                case_data['AtmosphericCondition'] = atmospheric.get('attrCatStr')\n",
    "            \n",
    "            # Estrai informazioni sulla luce naturale\n",
    "            natural_lighting = root.find(\".//PRECRASHVEHICLE/NATURAL_LIGHTING\")\n",
    "            if natural_lighting is not None:\n",
    "                case_data['NaturalLighting'] = natural_lighting.get('attrCatStr')\n",
    "            \n",
    "            # Aggiungi i dati di questo caso alla lista principale\n",
    "            data_list.append(case_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Errore nell'elaborazione del file {xml_file}: {str(e)}\")\n",
    "    \n",
    "    # Crea un DataFrame dai dati raccolti\n",
    "    df = pd.DataFrame(data_list)\n",
    "    \n",
    "    # Salva il DataFrame in un file CSV se richiesto\n",
    "    if output_csv:\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\"Dati salvati in {output_csv}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e61dae-dd0e-4f44-9289-e95a22338207",
   "metadata": {},
   "source": [
    "## HTML Data Extraction\n",
    "\n",
    "### HTML Parsing Challenges\n",
    "NMVCCS HTML files present specific challenges:\n",
    "- **Complex Structure**: Nested tables and divs with dynamic IDs\n",
    "- **Layout Variability**: Differences between file versions\n",
    "- **Hidden Data**: Information distributed across multiple sections\n",
    "- **Encoding Issues**: Special character handling\n",
    "\n",
    "### Extraction Strategy\n",
    "- **BeautifulSoup**: Robust HTML parsing\n",
    "- **Pattern Matching**: Regex for specific data extraction\n",
    "- **DOM Navigation**: Element search via attributes and positions\n",
    "- **Fallback Logic**: Handling structural variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26725ee7-9472-4fd6-b8ed-a70a7dd38207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_actuarial_data_from_html(html_file_or_directory, output_csv=None):\n",
    "    \"\"\"\n",
    "    Estrae dati rilevanti per analisi attuariale dai file HTML NMVCCS.\n",
    "    \n",
    "    Args:\n",
    "        html_file_or_directory: Un file HTML singolo o una directory contenente file HTML\n",
    "        output_csv: Opzionale, percorso dove salvare il CSV risultante\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame pandas con i dati estratti\n",
    "    \"\"\"\n",
    "    # Determina se l'input è un file o una directory\n",
    "    if os.path.isfile(html_file_or_directory):\n",
    "        html_files = [html_file_or_directory]\n",
    "    elif os.path.isdir(html_file_or_directory):\n",
    "        html_files = glob.glob(os.path.join(html_file_or_directory, \"*.html\"))\n",
    "    else:\n",
    "        raise ValueError(\"Il percorso fornito non è né un file né una directory valida\")\n",
    "    \n",
    "    # Lista per memorizzare i dati estratti\n",
    "    data_list = []\n",
    "    \n",
    "    # Processa ogni file HTML\n",
    "    for html_file in tqdm(html_files, desc=\"Elaborazione file HTML\"):\n",
    "        try:\n",
    "            # Leggi il file HTML\n",
    "            with open(html_file, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                html_content = file.read()\n",
    "            \n",
    "            # Utilizza BeautifulSoup per analizzare l'HTML\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            \n",
    "            # Dizionario per memorizzare i dati di questo caso\n",
    "            case_data = {}\n",
    "            \n",
    "            # Estrai numero caso dal titolo\n",
    "            case_title = soup.find('title')\n",
    "            if case_title:\n",
    "                case_match = re.search(r'NMVCCS Case (\\d+-\\d+-\\d+)', case_title.text)\n",
    "                if case_match:\n",
    "                    case_data['CaseID'] = case_match.group(1)\n",
    "            \n",
    "            # Estrai il riassunto del caso dal sommario\n",
    "            # Prima cerca la sezione \"Case Summary\"\n",
    "            summary_section = soup.find('a', {'name': 'Summary'})\n",
    "            if summary_section:\n",
    "                # Cerca la tabella Case Summary\n",
    "                summary_header = None\n",
    "                for table in soup.find_all('table'):\n",
    "                    for row in table.find_all('tr', {'class': 'heading'}):\n",
    "                        if 'Case Summary' in row.text:\n",
    "                            summary_header = table\n",
    "                            break\n",
    "                    if summary_header:\n",
    "                        break\n",
    "                \n",
    "                if summary_header:\n",
    "                    # Trova il div successivo che contiene il testo del riassunto\n",
    "                    summary_div = summary_header.find_next_sibling('div', id='indent')\n",
    "                    if summary_div and summary_div.find('td'):\n",
    "                        summary_text = summary_div.find('td').text.strip()\n",
    "                        summary_text = re.sub(r'\\s+', ' ', summary_text)\n",
    "                        case_data['CaseSummary'] = summary_text\n",
    "            \n",
    "            # Estrai informazioni di base dell'incidente\n",
    "            \n",
    "            crash_info_table = soup.find('th', string='Crash Overview')\n",
    "            if crash_info_table:\n",
    "                crash_table = crash_info_table.find_parent('table').find_next_sibling('div').find('table')\n",
    "                for row in crash_table.find_all('tr'):                    \n",
    "                    # Cerca in tutti i tag tr\n",
    "                    for tr_tag in row.find_all('tr'):\n",
    "                        cells = tr_tag.find_all(['th', 'td'])\n",
    "                        if len(cells) == 2:\n",
    "                            key = cells[0].text.strip()\n",
    "                            value = cells[1].text.strip()\n",
    "                            if key == 'Crash Level KABCOU':\n",
    "                                case_data['CrashSeverity'] = value\n",
    "                        if key == 'Case Number':\n",
    "                            case_data['CaseNum'] = value\n",
    "                        elif key == 'Date':\n",
    "                            case_data['CrashDate'] = value\n",
    "                        elif key == 'Day of Week':\n",
    "                            case_data['DayOfWeek'] = value\n",
    "                        elif key == 'PAR Time of Crash':\n",
    "                            case_data['CrashTime'] = value\n",
    "                        elif key == 'Crash Level KABCOU':\n",
    "                            case_data['CrashSeverity'] = value\n",
    "                            # Controlla anche direttamente le celle della riga corrente\n",
    "                    cells = row.find_all(['th', 'td'])\n",
    "                    if len(cells) == 2:\n",
    "                        key = cells[0].text.strip()\n",
    "                        value = cells[1].text.strip()\n",
    "                        if key == 'Case Number':\n",
    "                            case_data['CaseNum'] = value\n",
    "                        elif key == 'Date':\n",
    "                            case_data['CrashDate'] = value\n",
    "                        elif key == 'Day of Week':\n",
    "                            case_data['DayOfWeek'] = value\n",
    "                        elif key == 'PAR Time of Crash':\n",
    "                            case_data['CrashTime'] = value\n",
    "                        elif key == 'Crash Level KABCOU':\n",
    "                            case_data['CrashSeverity'] = value\n",
    "            # Cerca specificamente Crash Level KABCOU in tutto il documento\n",
    "            kabcou_elem = soup.find('th', string=lambda s: s and 'Crash Level KABCOU' in s)\n",
    "            if kabcou_elem and kabcou_elem.find_next_sibling('td'):\n",
    "                case_data['CrashSeverity'] = kabcou_elem.find_next_sibling('td').text.strip()\n",
    "            # Estrai informazioni sui veicoli\n",
    "            # Invece di cercare intestazioni, cerchiamo direttamente le sezioni di veicolo\n",
    "            for i in range(1, 10):  # Consideriamo fino a 10 veicoli (normalmente sono pochi)\n",
    "                # Trova tabella con le informazioni del veicolo\n",
    "                vehicle_section = soup.find('a', {'name': f'GV_Vehicle{i}'})\n",
    "                if not vehicle_section:\n",
    "                    continue  # Salta se questo veicolo non esiste\n",
    "                \n",
    "                vehicle_table = vehicle_section.find_next('table', {'class': 'output'})\n",
    "                if vehicle_table:\n",
    "                    for row in vehicle_table.find_all('tr'):\n",
    "                        cells = row.find_all(['th', 'td'])\n",
    "                        if len(cells) == 2:\n",
    "                            key = cells[0].text.strip()\n",
    "                            value = cells[1].text.strip()\n",
    "                            if key == 'Model Year':\n",
    "                                case_data[f'Vehicle{i}_Year'] = value\n",
    "                            elif key == 'Make':\n",
    "                                case_data[f'Vehicle{i}_Make'] = value\n",
    "                            elif key == 'Model':\n",
    "                                case_data[f'Vehicle{i}_Model'] = value\n",
    "                            elif key == 'Body Type':\n",
    "                                case_data[f'Vehicle{i}_BodyType'] = value\n",
    "                            elif key == 'Odometer Reading':\n",
    "                                # Estrai solo il valore numerico dell'odometro\n",
    "                                odometer_match = re.search(r'(\\d+)', value)\n",
    "                                if odometer_match:\n",
    "                                    case_data[f'Vehicle{i}_Odometer'] = odometer_match.group(1)\n",
    "                \n",
    "                # Estrai informazioni sui pneumatici\n",
    "                tire_section = soup.find('a', {'name': f'GV_Tire{i}'})\n",
    "                if tire_section:\n",
    "                    # Cerca tutte le tabelle di pneumatici dopo questa sezione\n",
    "                    current_element = tire_section\n",
    "                    tires_found = False\n",
    "                    while current_element and not (current_element.name == 'a' and current_element.get('name', '').startswith('GV_') and not current_element.get('name', '') == f'GV_Tire{i}'):\n",
    "                        current_element = current_element.find_next()\n",
    "                        \n",
    "                        # Verifica se questa è una tabella di pneumatici\n",
    "                        if current_element and current_element.name == 'table':\n",
    "                            headers = [th.text.strip() for th in current_element.find_all('th')]\n",
    "                            if len(headers) >= 5 and 'Location' in headers and 'Tread Depth' in headers:\n",
    "                                tires_found = True\n",
    "                                # Processa le righe della tabella (salta l'intestazione)\n",
    "                                for row in current_element.find_all('tr')[1:]:\n",
    "                                    cells = row.find_all('td')\n",
    "                                    if len(cells) >= 7:\n",
    "                                        location = cells[0].text.strip()\n",
    "                                        # Converti la posizione in un codice\n",
    "                                        loc_code = ''\n",
    "                                        if 'Left' in location:\n",
    "                                            loc_code = 'L'\n",
    "                                        elif 'Right' in location:\n",
    "                                            loc_code = 'R'\n",
    "                                        \n",
    "                                        if 'Front' in location:\n",
    "                                            loc_code += 'F'\n",
    "                                        elif 'Rear' in location:\n",
    "                                            loc_code += 'R'\n",
    "                                        \n",
    "                                        # Trova l'indice della colonna della profondità del battistrada\n",
    "                                        tread_depth_idx = headers.index('Tread Depth (mm)') if 'Tread Depth (mm)' in headers else 6\n",
    "                                        pressure_idx = tread_depth_idx + 1\n",
    "                                        \n",
    "                                        if tread_depth_idx < len(cells):\n",
    "                                            tread_depth = cells[tread_depth_idx].text.strip()\n",
    "                                            if tread_depth and re.search(r'\\d+', tread_depth):\n",
    "                                                case_data[f'Vehicle{i}_Tire{loc_code}_Depth'] = re.search(r'\\d+', tread_depth).group(0)\n",
    "                                        \n",
    "                                        if pressure_idx < len(cells):\n",
    "                                            pressure = cells[pressure_idx].text.strip()\n",
    "                                            pressure_match = re.search(r'\\d+', pressure)\n",
    "                                            if pressure_match:\n",
    "                                                case_data[f'Vehicle{i}_Tire{loc_code}_Pressure'] = pressure_match.group(0)\n",
    "                \n",
    "                # Estrai informazioni CDC (Collision Deformation Classification)\n",
    "                cdc_section = soup.find('a', {'name': f'GV_CDC{i}'})\n",
    "                if cdc_section:\n",
    "                    cdc_table = cdc_section.find_next('table', {'class': 'output'})\n",
    "                    if cdc_table:\n",
    "                        for row in cdc_table.find_all('tr'):\n",
    "                            cells = row.find_all(['th', 'td'])\n",
    "                            if len(cells) == 2 and 'Extent' in cells[0].text:\n",
    "                                case_data[f'Vehicle{i}_DamageExtent'] = cells[1].text.strip()\n",
    "                \n",
    "                # Estrai informazioni sull'autista\n",
    "                driver_section = soup.find('a', {'name': f'OC_OccupantV{i}O1'})\n",
    "                if driver_section:\n",
    "                    driver_table = driver_section.find_next('table', {'class': 'output'})\n",
    "                    if driver_table:\n",
    "                        for row in driver_table.find_all('tr'):\n",
    "                            cells = row.find_all(['th', 'td'])\n",
    "                            if len(cells) == 2:\n",
    "                                key = cells[0].text.strip()\n",
    "                                value = cells[1].text.strip()\n",
    "                                if key == 'Age':\n",
    "                                    age_match = re.search(r'(\\d+)', value)\n",
    "                                    if age_match:\n",
    "                                        case_data[f'Vehicle{i}_DriverAge'] = age_match.group(1)\n",
    "                                elif key == 'Sex':\n",
    "                                    case_data[f'Vehicle{i}_DriverSex'] = value\n",
    "                                elif key == 'Occupant KABCOU Rating':\n",
    "                                    case_data[f'Vehicle{i}_DriverInjurySeverity'] = value\n",
    "                \n",
    "                # Estrai informazioni precrash per il veicolo\n",
    "                precrash_section = soup.find('a', {'name': f'PA_Precrash{i}'})\n",
    "                if precrash_section:\n",
    "                    # Naviga attraverso tutti i divs e le tabelle dopo la sezione precrash\n",
    "                    current_element = precrash_section\n",
    "                    while current_element and not (current_element.name == 'a' and 'PA_SupportData' in current_element.get('name', '')):\n",
    "                        current_element = current_element.find_next()\n",
    "                        \n",
    "                        # Cerca informazioni sul Critical Event\n",
    "                        if current_element and current_element.name == 'tr' and current_element.find('th'):\n",
    "                            header = current_element.find('th').text.strip()\n",
    "                            if header == 'Critical Pre-Crash Event':\n",
    "                                value = current_element.find('td').text.strip()\n",
    "                                case_data[f'Vehicle{i}_CriticalEvent'] = value\n",
    "                            elif header == 'Critical Reason for Critical Pre-Crash Event':\n",
    "                                value = current_element.find('td').text.strip()\n",
    "                                case_data[f'Vehicle{i}_CriticalReason'] = value\n",
    "                \n",
    "                # Estrai informazioni sulla fatica del conducente\n",
    "                support_data_section = soup.find('a', {'name': f'PA_SupportData{i}'})\n",
    "                if support_data_section:\n",
    "                    next_elems = support_data_section.find_next_siblings(['div', 'table'])\n",
    "                    for elem in next_elems:\n",
    "                        if 'Fatigue' in elem.text:\n",
    "                            fatigue_tables = elem.find_all('table')\n",
    "                            for table in fatigue_tables:\n",
    "                                for row in table.find_all('tr'):\n",
    "                                    cells = row.find_all(['th', 'td'])\n",
    "                                    if len(cells) == 2 and 'Driver Fatigue' in cells[0].text:\n",
    "                                        case_data[f'Vehicle{i}_DriverFatigue'] = cells[1].text.strip()\n",
    "            \n",
    "            # Estrai informazioni sull'ambiente/strada\n",
    "            roadway_sections = soup.find_all(string=lambda s: s and 'Roadway' in s)\n",
    "            for roadway_text in roadway_sections:\n",
    "                parent = roadway_text.parent\n",
    "                if parent and parent.name == 'td':\n",
    "                    roadway_table = parent.find_parent('table')\n",
    "                    if roadway_table:\n",
    "                        next_div = roadway_table.find_next_sibling('div', id='indent')\n",
    "                        if next_div:\n",
    "                            road_table = next_div.find('table')\n",
    "                            if road_table:\n",
    "                                for row in road_table.find_all('tr'):\n",
    "                                    cells = row.find_all(['th', 'td'])\n",
    "                                    if len(cells) == 2:\n",
    "                                        key = cells[0].text.strip()\n",
    "                                        value = cells[1].text.strip()\n",
    "                                        if 'Type of Road Surface' in key:\n",
    "                                            case_data['RoadSurfaceType'] = value\n",
    "                                        elif 'Condition of Road Surface' in key:\n",
    "                                            case_data['RoadSurfaceCondition'] = value\n",
    "                                        elif 'Roadway Horizontal Alignment' in key:\n",
    "                                            case_data['RoadwayAlignment'] = value\n",
    "                                        elif 'Roadway Vertical Profile' in key:\n",
    "                                            case_data['RoadwayVerticalProfile'] = value\n",
    "            \n",
    "            # Estrai informazioni sulle condizioni atmosferiche\n",
    "            for table in soup.find_all('table'):\n",
    "                for row in table.find_all('tr', {'class': 'highlightrow'}):\n",
    "                    if row.find('th') and 'Atmospheric Condition' in row.find('th').text:\n",
    "                        atmospheric_value = row.find('td')\n",
    "                        if atmospheric_value:\n",
    "                            case_data['AtmosphericCondition'] = atmospheric_value.text.strip()\n",
    "            \n",
    "            # Aggiungi i dati di questo caso alla lista principale\n",
    "            data_list.append(case_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Errore nell'elaborazione del file {html_file}: {str(e)}\")\n",
    "    \n",
    "    # Crea un DataFrame dai dati raccolti\n",
    "    df = pd.DataFrame(data_list)\n",
    "    \n",
    "    # Salva il DataFrame in un file CSV se richiesto\n",
    "    if output_csv:\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\"Dati salvati in {output_csv}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a93744-c1b5-44b8-85ae-ee66b337171c",
   "metadata": {},
   "source": [
    "## Unified Extraction Function\n",
    "\n",
    "### Automatic Format Handling\n",
    "The `extract_actuarial_data()` function automatically handles:\n",
    "- **Single Files**: XML or HTML\n",
    "- **Directories**: Mixed with XML and HTML\n",
    "- **Combination**: Automatic result merging\n",
    "- **Validation**: Format and path checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00a77ff-dced-48eb-a67d-ece70aaec89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_actuarial_data(file_or_directory, output_csv=None):\n",
    "    \"\"\"\n",
    "    Estrae dati rilevanti per analisi attuariale dai file NMVCCS (XML o HTML).\n",
    "    \n",
    "    Args:\n",
    "        file_or_directory: Un file singolo o una directory contenente file\n",
    "        output_csv: Opzionale, percorso dove salvare il CSV risultante\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame pandas con i dati estratti\n",
    "    \"\"\"\n",
    "    # Determina se l'input è un file o una directory\n",
    "    if os.path.isfile(file_or_directory):\n",
    "        file_ext = os.path.splitext(file_or_directory)[1].lower()\n",
    "        if file_ext == '.xml':\n",
    "            return extract_actuarial_data_from_xml(file_or_directory, output_csv)\n",
    "        elif file_ext in ['.html', '.htm']:\n",
    "            return extract_actuarial_data_from_html(file_or_directory, output_csv)\n",
    "        else:\n",
    "            raise ValueError(f\"Formato file non supportato: {file_ext}\")\n",
    "    elif os.path.isdir(file_or_directory):\n",
    "        # Trova tutti i file XML e HTML nella directory\n",
    "        xml_files = glob.glob(os.path.join(file_or_directory, \"*.xml\"))\n",
    "        html_files = glob.glob(os.path.join(file_or_directory, \"*.html\"))\n",
    "        htm_files = glob.glob(os.path.join(file_or_directory, \"*.htm\"))\n",
    "        html_files.extend(htm_files)  # Combina i file .html e .htm\n",
    "        \n",
    "        # Elabora tutti i file e unisci i risultati\n",
    "        data_frames = []\n",
    "        \n",
    "        if xml_files:\n",
    "            xml_df = extract_actuarial_data_from_xml(file_or_directory)\n",
    "            data_frames.append(xml_df)\n",
    "            print(f\"Estratti dati da {len(xml_files)} file XML\")\n",
    "        \n",
    "        if html_files:\n",
    "            html_df = extract_actuarial_data_from_html(file_or_directory)\n",
    "            data_frames.append(html_df)\n",
    "            print(f\"Estratti dati da {len(html_files)} file HTML\")\n",
    "        \n",
    "        if not data_frames:\n",
    "            raise ValueError(\"Nessun file XML o HTML trovato nella directory\")\n",
    "        \n",
    "        # Unisci tutti i DataFrame\n",
    "        combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "        \n",
    "        # Salva il DataFrame in un file CSV se richiesto\n",
    "        if output_csv:\n",
    "            combined_df.to_csv(output_csv, index=False)\n",
    "            print(f\"Dati salvati in {output_csv}\")\n",
    "        \n",
    "        return combined_df\n",
    "    else:\n",
    "        raise ValueError(\"Il percorso fornito non è né un file né una directory valida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe40960-4f42-422d-964c-7c72b29dc03f",
   "metadata": {},
   "source": [
    "## Extraction Execution\n",
    "\n",
    "### Single File Test\n",
    "Testing the function on a sample HTML file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52758594-ed8e-45d4-8e0e-0efba8eef60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Elaborazione file HTML: 100%|██████████| 1/1 [00:00<00:00,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dati salvati in output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Esempio di utilizzo\n",
    "if __name__ == \"__main__\":\n",
    "    # Per elaborare un singolo file XML\n",
    "    # df = extract_actuarial_data(\"path/to/CaseForm.xml\", \"output.csv\")\n",
    "    \n",
    "    # Per elaborare un singolo file HTML\n",
    "    df = extract_actuarial_data(\"2005045588642_direct2.html\", \"output.csv\")\n",
    "    \n",
    "    # Per elaborare tutti i file XML e HTML in una directory\n",
    "    # df = extract_actuarial_data(\"path/to/directory\", \"output.csv\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9d4a51-a268-4d1a-8344-665615b4ee3e",
   "metadata": {},
   "source": [
    "## Bulk Processing\n",
    "\n",
    "### Complete Directory Processing\n",
    "Processing all XML files in the `nmvccs_xml_files` directory:\n",
    "- **Progress Bar**: Advanced progress monitoring with tqdm\n",
    "- **Error Handling**: Managing corrupted or malformed files\n",
    "- **Memory Management**: Efficient processing of thousands of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dbef515-3dc4-4313-aea1-7e5048f18a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Elaborazione file XML: 100%|██████████| 4/4 [00:00<00:00, 181.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estratti dati da 4 file XML\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Elaborazione file HTML: 100%|██████████| 6926/6926 [49:55<00:00,  2.31it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estratti dati da 6926 file HTML\n",
      "Dati salvati in output.csv\n"
     ]
    }
   ],
   "source": [
    "df = extract_actuarial_data(\"nmvccs_xml_files\", \"output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aca9ad-f622-4c42-8262-b3b308447779",
   "metadata": {},
   "source": [
    "## Data Saving and Cleaning\n",
    "\n",
    "### Output Format\n",
    "- **DB.csv**: Main database with `;` separator (European standard)\n",
    "- **Deduplication**: Automatic duplicate removal\n",
    "- **Encoding**: UTF-8 for international compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c051242-5016-41eb-a91d-8bbbb5dcad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('DB.csv',sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47ee7217-d265-4525-9873-3fdc42491dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b238048-5d23-4671-9d4f-3bcb9464473a",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Generated Dataset\n",
    "The process produces a structured dataset with:\n",
    "- **Actuarial Variables**: 50+ columns for risk analysis\n",
    "- **Complete Coverage**: All crash aspects\n",
    "- **Data Quality**: Standardization and validation\n",
    "- **Standard Format**: CSV compatible with actuarial systems\n",
    "\n",
    "### Possible Uses\n",
    "- **Risk Assessment**: Insurance risk profiling\n",
    "- **Claims Analysis**: Claim cost analysis\n",
    "- **Underwriting**: Pricing model development\n",
    "- **Safety Research**: Accident prevention research"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
